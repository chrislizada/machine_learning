{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f464a6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Text\n",
    "\n",
    "text_data = [\"  Interrobang. By Aishwarya Henriette     \",\n",
    "             \"Parking And Going. By Karl Gautier\",\n",
    "             \"  Today Is The night. By Jarek Prakash    \"]\n",
    "\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "strip_whitespace\n",
    "\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "remove_periods\n",
    "\n",
    "# Also try a custom function for transformation\n",
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ec1a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5050035d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing and Cleaning HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"<div class='full_name'>\"\\\n",
    "    \"<span style='font-weight:bold'>Masego\"\\\n",
    "    \"</span> Azra</div>\"\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "soup.find(\"div\", {\"class\": \"full_name\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4233b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Punctuation\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "             '10000% Agree!!!! #LoveIT',\n",
    "             'Right?!?!']\n",
    "\n",
    "punctuation = dict.fromkeys(\n",
    "    (i for i in range(sys.maxunicode)\n",
    "    if unicodedata.category(chr(i)).startswith('P')\n",
    "    ),\n",
    "    None\n",
    ")\n",
    "\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a74788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('all', download_dir='C:/nltk_data')\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "word_tokenize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ed7d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aea46dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenize_words = ['i',\n",
    "                  'am',\n",
    "                  'going',\n",
    "                  'to',\n",
    "                  'go',\n",
    "                  'the',\n",
    "                  'store',\n",
    "                  'and',\n",
    "                  'park']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "[word for word in tokenize_words if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92a8f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming Words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenize_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenize_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572f861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagging Parts of Speech\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "print(text_tagged)\n",
    "\n",
    "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0beb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PRP', 'VBP', 'VBG', 'DT', 'NN', 'IN', 'NN'], ['JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN'], ['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN']]\n",
      "[[1 1 0 1 0 1 1 1 0]\n",
      " [1 0 1 1 0 0 0 0 1]\n",
      " [1 0 1 1 1 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "          \"Political science is an amazing field\",\n",
    "          \"San Francisco is an awesome city\"]\n",
    "\n",
    "tagged_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_tag = pos_tag(word_tokenize(tweet))\n",
    "    # print(tweet_tag)\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "\n",
    "print(tagged_tweets)\n",
    "print(one_hot_multi.fit_transform(tagged_tweets))\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d60fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Elon Musk, Twitter, 21B)\n",
      "Elon Musk,PERSON\n",
      "Twitter,PERSON\n",
      "21B,MONEY\n"
     ]
    }
   ],
   "source": [
    "# Performing Named-Entity Recognition\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money\")\n",
    "\n",
    "print(doc.ents)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf040452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 2 0 0 1 0]\n",
      " [0 1 0 0 0 1 0 1]\n",
      " [1 0 1 0 1 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
       "       'sweden'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding Text as a Bag of Words\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "bag_of_words\n",
    "\n",
    "print(bag_of_words.toarray())\n",
    "\n",
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e663b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                              stop_words='english',\n",
    "                              vocabulary=['brazil'])\n",
    "\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "\n",
    "bag.toarray()\n",
    "\n",
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d41494f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weighting Word Importance\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "feature_matrix\n",
    "\n",
    "feature_matrix.toarray()\n",
    "\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb412af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.str_('I love Brazil. Brazil!'), np.float64(0.9999999999999999)), (np.str_('Germany beats both'), np.float64(0.0)), (np.str_('Sweden is best'), np.float64(0.0))]\n"
     ]
    }
   ],
   "source": [
    "# Using Text Vectors to Calculate Text Similarity in a Search Query\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "text = \"I love Brazil. Brazil!\"\n",
    "vector = tfidf.transform([text])\n",
    "\n",
    "cosine_similarities = linear_kernel(vector, feature_matrix).flatten()\n",
    "\n",
    "related_doc_indices = cosine_similarities.argsort()[:-10:-1]\n",
    "\n",
    "print([(text_data[i], cosine_similarities[i]) for i in related_doc_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406d2cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9998020529747009}] [{'label': 'POSITIVE', 'score': 0.9992170333862305}]\n"
     ]
    }
   ],
   "source": [
    "# Using a Sentiment Analysis Classifier\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "sentiment_1 = classifier(\"I hate machine learning! It's the absolute worst.\")\n",
    "sentiment_2 = classifier(\n",
    "    \"Machine learning is absolute\"\n",
    "    \"bees knees I love it so much!\")\n",
    "\n",
    "print(sentiment_1, sentiment_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
